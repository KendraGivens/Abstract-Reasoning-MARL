{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ac3788a-d578-46e6-b337-033a1b0f1579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import numpy as np\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from enum import Enum, auto\n",
    "import yaml\n",
    "from types import SimpleNamespace as SN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6fb1932-d6a5-4aa1-8f21-ea1dd7c1025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get config of qmix algorithim \n",
    "with open('qmix.yaml', 'r') as f:\n",
    "    qmix_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "# get config of environment\n",
    "with open('env.yaml', 'r') as f:\n",
    "    env_config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "502f7968-ae41-4cf5-a623-5977887aa4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actions(Enum):\n",
    "    NO_OP = 0\n",
    "    MOVE_UP = auto()\n",
    "    MOVE_DOWN = auto()\n",
    "    MOVE_LEFT = auto()\n",
    "    MOVE_RIGHT = auto()\n",
    "    \n",
    "    @property\n",
    "    def delta(self):\n",
    "        if self == self.NO_OP:\n",
    "            return (0, 0)\n",
    "        if self == self.MOVE_UP:\n",
    "            return (-1, 0)\n",
    "        if self == self.MOVE_DOWN:\n",
    "            return (1, 0)\n",
    "        if self == self.MOVE_LEFT:\n",
    "            return (0, -1)\n",
    "        if self == self.MOVE_RIGHT:\n",
    "            return (0, 1)\n",
    "\n",
    "    @property\n",
    "    def one_hot(self):\n",
    "        out_dim = len(Actions)\n",
    "        y_one_hot = torch.zeros(out_dim)\n",
    "        y_one_hot[self.value] = 1\n",
    "        return y_one_hot.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a28c36a-8b7e-43d4-bb49-e937de7799cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnv():\n",
    "    def __init__(self, rows, cols, n_agents, state_shape):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.n_agents = n_agents\n",
    "\n",
    "        self.state_shape = state_shape\n",
    "        self.obs_shape = (n_agents+2)*self.state_shape\n",
    "        self.action_shape = len(Actions)\n",
    "        self.agent_in_shape = self.obs_shape + self.action_shape\n",
    "\n",
    "        self.block = None\n",
    "        self.goal = None\n",
    "        self.agent = None\n",
    "\n",
    "        self.populate_grid()\n",
    "        \n",
    "    def get_global_state(self):\n",
    "        return torch.flatten(nn.functional.one_hot(torch.tensor([p[0]*self.cols+p[1] for p in (self.goal, self.block, self.agent.pos)]), self.rows*self.cols))\n",
    "        \n",
    "    def populate_grid(self):\n",
    "        positions = np.random.choice(self.rows*self.cols, 1+2, replace=False)\n",
    "\n",
    "        self.goal = (positions[0]//self.cols, positions[0]%self.cols)\n",
    "        self.block = (positions[1]//self.cols, positions[1]%self.cols)\n",
    "        self.agent = Agent(self, (positions[2]//self.cols, positions[2]%self.cols))\n",
    "\n",
    "    def vizualize_grid(self): \n",
    "        grid = [list(\".\"*self.cols) for _ in range(self.rows)]\n",
    "        \n",
    "        grid[self.goal[0]][self.goal[1]] = \"G\"\n",
    "        grid[self.block[0]][self.block[1]]= \"B\"\n",
    "        \n",
    "        grid[self.agent.pos[0]][self.agent.pos[1]] = \"A\"\n",
    "        \n",
    "        return '\\n'.join([' '.join(row) for row in grid])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.vizualize_grid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad68ab0f-1364-4363-a813-d62ce70707a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentModel(nn.Module): \n",
    "    def __init__(self, input_shape, embed_dim, num_actions): # input shape is shape of a replay buffer\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_actions = num_actions\n",
    "        self.linear1 = nn.Linear(input_shape, self.embed_dim)\n",
    "        self.rnn = nn.GRUCell(self.embed_dim, self.embed_dim)\n",
    "        self.linear2 = nn.Linear(self.embed_dim, self.num_actions)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return self.linear1.weight.new(1, self.embed_dim).zero_()\n",
    "    \n",
    "    def forward(self, inputs, hidden_in):\n",
    "        x = nn.functional.relu(self.linear1(inputs)) \n",
    "        h_in = torch.reshape(hidden_in, (-1, self.embed_dim))\n",
    "        hidden_out = self.rnn(x, h_in)\n",
    "        q_values = self.linear2(hidden_out)\n",
    "        return q_values, hidden_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e3e8302-ad5c-42a1-8525-16d96cdef573",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, grid, pos):\n",
    "        self.pos = pos\n",
    "        self.grid = grid\n",
    "        self.embed_dim = 64\n",
    "        self.hidden_states = torch.zeros((self.embed_dim,)) \n",
    "        self.local_model = AgentModel(self.grid.agent_in_shape, self.embed_dim, len(Actions))\n",
    "        self.prev_action = Actions.NO_OP\n",
    "        self.epsilon = 0.1\n",
    "        self.alpha = 0.01\n",
    "        self.gamma = 0.01 \n",
    "        self.loss = nn.HuberLoss()\n",
    "        self.optimizer = torch.optim.Adam([{'params': self.local_model.parameters(), 'lr': self.alpha}])\n",
    "        self.agent_in = None\n",
    "        self.target_qvalues = None\n",
    "        \n",
    "    def get_reward(self):\n",
    "        if self.grid.block == self.grid.goal:\n",
    "            print(\"Goal Reached!\")\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def get_correct_qvalue(self, pred_max_value, reward, current_max_value):\n",
    "        q_value = pred_max_value + self.alpha * (reward + self.gamma * current_max_value - pred_max_value)\n",
    "        return q_value\n",
    "\n",
    "    def take_action(self, action):\n",
    "        future_position = self.pos\n",
    "        future_position += np.array(action.delta)\n",
    "        future_position %= [self.grid.rows, self.grid.cols]\n",
    "\n",
    "        if list(future_position) == self.grid.block:\n",
    "            self.grid.block += np.array(action.delta)\n",
    "            self.grid.block %= [self.grid.rows, self.grid.cols]\n",
    "        else:\n",
    "            self.pos = future_position\n",
    "            \n",
    "    def step(self, vizualize):\n",
    "        if vizualize:\n",
    "            clear_output(wait=True)\n",
    "            print(self.grid.vizualize_grid())\n",
    "            \n",
    "        current_state = self.grid.get_global_state()\n",
    "        self.agent_in = torch.unsqueeze(torch.cat((self.grid.get_global_state(), self.prev_action.one_hot)), 0)\n",
    "        pred_qvalues, self.hidden_states = self.local_model(self.agent_in, self.hidden_states)\n",
    "    \n",
    "        pred_max_index = np.argmax(torch.detach(pred_qvalues).numpy())\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "           action = random.choice(list(Actions))\n",
    "        else:\n",
    "            action = Actions(pred_max_index)\n",
    "            \n",
    "        self.take_action(action)\n",
    "        self.prev_action = action\n",
    "        future_state = self.grid.get_global_state()\n",
    "        future_in = torch.unsqueeze(torch.cat((future_state, self.prev_action.one_hot)), 0)\n",
    "\n",
    "        reward = self.get_reward()        \n",
    "        future_qvalues, _  = self.local_model(future_in, self.hidden_states)\n",
    "        future_max_index = np.argmax(torch.detach(future_qvalues).numpy())\n",
    "        \n",
    "        target_qvalue = self.get_correct_qvalue(pred_qvalues[0][pred_max_index], reward, future_qvalues[0][future_max_index])\n",
    "        self.target_qvalues = pred_qvalues\n",
    "        self.target_qvalues[0][pred_max_index] = target_qvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a62e8578-d795-433d-a2d9-1c07a2d2a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1189623f-b7d9-473d-8cc1-32300fa5bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner():\n",
    "    def __init__(self, grid, batch):\n",
    "        self.grid = grid\n",
    "        self.batch = batch\n",
    "\n",
    "    def run_episode(self, n_timesteps):\n",
    "        t = 0\n",
    "        while self.grid.block != self.grid.goal and t < n_timesteps: \n",
    "            self.grid.agent.step(True)\n",
    "            print(t)\n",
    "            t += 1\n",
    "        \n",
    "    # def train(self):\n",
    "    #     torch.autograd.set_detect_anomaly(True)\n",
    "    #     for inputs, targets in zip([self.agent_in], [self.target_qvalues]):  \n",
    "    #         self.optimizer.zero_grad()  # Zero the gradients\n",
    "    #         outputs, _ = g.agent.local_model(inputs, self.hidden_states)  # Forward pass\n",
    "    #         loss = self.loss(outputs, targets)  # Compute loss\n",
    "    #         loss.backward()  # Backward pass\n",
    "    #         self.optimizer.step()  # Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "13185544-0cc6-4c7c-8fd8-2a49a780ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GridEnv(6, 4, 1, 6*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7c2d9aa8-a129-44fa-9011-c9ece983b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "qlearner = QLearner(g, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b0498afe-92c1-4618-8697-0d4c23e3f710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . .\n",
      ". B G .\n",
      "A . . .\n",
      ". . . .\n",
      ". . . .\n",
      ". . . .\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "qlearner.run_episode(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddb301e-efa7-43cf-a415-7aaaa5be85bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class AgentController():\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87fbe64-e769-4896-b33a-93ebabb55a37",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# class EpisodeBatch():\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "    \n",
    "#     def return_batch(self):\n",
    "#         return 0\n",
    "\n",
    "#     # def max_t_filled(self):\n",
    "#     #     return torch.sum(self.data.transition_data.[\"filled\"], 1).max(0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e689dbe-d517-43bd-a18e-909c493895dc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# class ReplayBuffer(EpisodeBatch):\n",
    "#     def __init__(self, buffer_size):\n",
    "#         self.buffer_size = buffer_size\n",
    "#         self.episodes_in_buffer = 0\n",
    "\n",
    "#     def insert_episode_batch(self): \n",
    "#         pass\n",
    "\n",
    "#     def can_sample(self, batch_size):\n",
    "#         return self.episodes_in_buffer >= batch_size\n",
    "\n",
    "#     def sample(self, batch_size):\n",
    "#         assert self.can_sample(batch_size)\n",
    "        \n",
    "#         if self.episodes_in_buffer == batch_size: # return one batch\n",
    "#             return self[:batch_size]\n",
    "#         else: # sample uniformly\n",
    "#             n_sampled_episodes = np.random.choice(self.episodes_in_batch, batch_size, replace=False)\n",
    "#             return self[:n_sampled_episodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd774c3a-5f77-4d78-8d2e-735ef9751f2e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# class EpisodeRunner():\n",
    "#     def __init__(self, data):\n",
    "#         self.batcher = EpisodeBatch()\n",
    "\n",
    "#         if data is not None:\n",
    "#             self.data = data\n",
    "#         else:\n",
    "#             data = SN()\n",
    "\n",
    "    \n",
    "#     def get_batch(self):\n",
    "#         batch = self.batcher.return_batch()\n",
    "#         return batch\n",
    "\n",
    "#     def run_episode(self, training_mode):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba66a46-caa7-4b26-b9bd-ad8030b15b3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# class Runner(): \n",
    "#     def __init__(self, num_episodes):\n",
    "#         self.rows = 6\n",
    "#         self.cols = 4\n",
    "#         self.n_agents = 1\n",
    "#         # self.batch = torch.tensor(g.get_global_state() + torch.unsqueeze(torch.tensor_size == 1))\n",
    "#         self.n_actions = len(Actions)\n",
    "#         self.state_shape = self.rows*self.cols\n",
    "\n",
    "#         self.batch_size = 1\n",
    "        \n",
    "#         self.env = GridEnv(self.rows, self.cols, self.n_agents, self.state_shape)\n",
    "#         self.replay_buffer = ReplayBuffer(self.batch_size)\n",
    "#         self.agent_controller = AgentController()\n",
    "#         self.learner = QLearner()\n",
    "#         self.episode_runner = EpisodeRunner(None)\n",
    "\n",
    "#         self.max_timesteps = 20\n",
    "#         self.current_t = 0\n",
    "\n",
    "#     def train(self):\n",
    "#         episode = 0\n",
    "#         while self.current_t <= self.max_timesteps:\n",
    "#             # run for one episode\n",
    "#             episode = self.episode_runner.run_episode(training_mode=True)\n",
    "#             #insert episode into replay buffer\n",
    "#             self.replay_buffer.insert_episode_batch()\n",
    "\n",
    "#             if self.replay_buffer.can_sample(self.batch_size):\n",
    "#                 episode_sample = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "#                 max_eps_t = episode_sample.max_t_filled() \n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e6b0e7-2f22-478b-a213-5085d76dbe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = Runner(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c49e88-0fe6-4fe2-879f-7669a8e463a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324bf820-bb74-4683-b72f-3004c7af14cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

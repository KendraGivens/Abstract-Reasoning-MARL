{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ac3788a-d578-46e6-b337-033a1b0f1579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import numpy as np\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from enum import Enum, auto\n",
    "import yaml\n",
    "from types import SimpleNamespace as SN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6fb1932-d6a5-4aa1-8f21-ea1dd7c1025b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# get config of qmix algorithim \n",
    "with open('qmix.yaml', 'r') as f:\n",
    "    qmix_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "# get config of environment\n",
    "with open('env.yaml', 'r') as f:\n",
    "    env_config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "502f7968-ae41-4cf5-a623-5977887aa4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actions(Enum):\n",
    "    NO_OP = 0\n",
    "    MOVE_UP = auto()\n",
    "    MOVE_DOWN = auto()\n",
    "    MOVE_LEFT = auto()\n",
    "    MOVE_RIGHT = auto()\n",
    "\n",
    "    @property\n",
    "    def delta(self):\n",
    "        if self == self.NO_OP:\n",
    "            return (0, 0)\n",
    "        if self == self.MOVE_UP:\n",
    "            return (-1, 0)\n",
    "        if self == self.MOVE_DOWN:\n",
    "            return (1, 0)\n",
    "        if self == self.MOVE_LEFT:\n",
    "            return (0, -1)\n",
    "        if self == self.MOVE_RIGHT:\n",
    "            return (0, 1)\n",
    "\n",
    "    @property\n",
    "    def one_hot(self):\n",
    "        out_dim = len(Actions)\n",
    "        y_one_hot = torch.zeros(out_dim)\n",
    "        y_one_hot[self.value] = 1\n",
    "        return y_one_hot.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "4a28c36a-8b7e-43d4-bb49-e937de7799cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnv():\n",
    "    def __init__(self, rows, cols, n_agents):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.n_agents = n_agents\n",
    "\n",
    "        self.state_shape = self.rows*self.cols\n",
    "        self.obs_shape = (n_agents+2)*self.state_shape\n",
    "        self.action_shape = len(Actions)\n",
    "        self.agent_in_shape = self.obs_shape + self.action_shape\n",
    "\n",
    "        self.block = None\n",
    "        self.goal = None\n",
    "        self.agent = None\n",
    "\n",
    "        self.reset()\n",
    "        \n",
    "    def get_global_state(self):\n",
    "        return torch.flatten(nn.functional.one_hot(torch.tensor([p[0]*self.cols+p[1] for p in (self.goal, self.block, self.agent.pos)]), self.rows*self.cols))\n",
    "        \n",
    "    def reset(self):\n",
    "        positions = np.random.choice(self.rows*self.cols, 1+2, replace=False)\n",
    "\n",
    "        self.goal = (positions[0]//self.cols, positions[0]%self.cols)\n",
    "        self.block = (positions[1]//self.cols, positions[1]%self.cols)\n",
    "        self.agent = Agent(self, (positions[2]//self.cols, positions[2]%self.cols))\n",
    "\n",
    "    def get_reward(self):\n",
    "        if list(self.block) == list(self.goal):\n",
    "            return 0, True\n",
    "        else:\n",
    "            return -1, False\n",
    "\n",
    "    def vizualize_grid(self): \n",
    "        grid = [list(\".\"*self.cols) for _ in range(self.rows)]\n",
    "        \n",
    "        grid[self.goal[0]][self.goal[1]] = \"G\"\n",
    "        grid[self.block[0]][self.block[1]]= \"B\"\n",
    "        \n",
    "        grid[self.agent.pos[0]][self.agent.pos[1]] = \"A\"\n",
    "        \n",
    "        return '\\n'.join([' '.join(row) for row in grid])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.vizualize_grid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "ad68ab0f-1364-4363-a813-d62ce70707a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentModel(nn.Module): \n",
    "    def __init__(self, input_shape, embed_dim, n_actions): # input shape is shape of a replay buffer\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_actions = n_actions\n",
    "        self.linear1 = nn.Linear(input_shape, self.embed_dim) \n",
    "        self.rnn = nn.GRUCell(self.embed_dim, self.embed_dim)\n",
    "        self.linear2 = nn.Linear(self.embed_dim, self.n_actions)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return self.linear1.weight.new(1, self.embed_dim).zero_()\n",
    "    \n",
    "    def forward(self, inputs, hidden_in): \n",
    "        x = nn.functional.relu(self.linear1(inputs)) \n",
    "        h_in = torch.reshape(hidden_in, (-1, self.embed_dim))\n",
    "        hidden_out = self.rnn(x, h_in)\n",
    "        q_values = self.linear2(hidden_out)\n",
    "        return q_values, hidden_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "5e3e8302-ad5c-42a1-8525-16d96cdef573",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, grid, pos):\n",
    "        self.pos = pos\n",
    "        self.grid = grid\n",
    "        self.embed_dim = 64\n",
    "        self.hidden_states = torch.zeros((self.embed_dim,)) \n",
    "        self.local_model = AgentModel(self.grid.agent_in_shape, self.embed_dim, len(Actions))\n",
    "        self.prev_action = Actions.NO_OP\n",
    "        self.epsilon = 0.1\n",
    "        self.gamma = 0.01 \n",
    "\n",
    "    def get_target_qvalue(self, reward, future_q_value):\n",
    "        q_value = reward + self.gamma * future_q_value\n",
    "        return q_value\n",
    "\n",
    "    def take_action(self, action):\n",
    "        future_position = self.pos + np.array(action.delta)\n",
    "        future_position %= [self.grid.rows, self.grid.cols]\n",
    "        \n",
    "        if list(future_position) == list(self.grid.block):\n",
    "            self.grid.block += np.array(action.delta)\n",
    "            self.grid.block %= [self.grid.rows, self.grid.cols]\n",
    "        else:\n",
    "            self.pos = future_position\n",
    "             \n",
    "    def step(self):\n",
    "        tstep_history = []\n",
    "        terminated = False\n",
    "        goal_reached = False\n",
    "\n",
    "        current_state = self.grid.get_global_state()\n",
    "        ep_prev_action = self.prev_action.one_hot\n",
    "        \n",
    "        agent_in = torch.unsqueeze(torch.cat((current_state, self.prev_action.one_hot)), 0)\n",
    "        pred_qvalues, self.hidden_states = self.local_model(agent_in, self.hidden_states)\n",
    "        pred_q_index = torch.argmax(pred_qvalues)\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.choice(list(Actions))\n",
    "            pred_q_index = action.value\n",
    "        else:\n",
    "            action = Actions(int(pred_q_index))\n",
    "            \n",
    "        self.take_action(action)\n",
    "        self.prev_action = action\n",
    "        future_state = self.grid.get_global_state()\n",
    "        future_in = torch.unsqueeze(torch.cat((future_state, self.prev_action.one_hot)), 0)\n",
    "\n",
    "        reward, goal_reached = self.grid.get_reward()        \n",
    "        future_qvalues, _  = self.local_model(future_in, self.hidden_states)\n",
    "        future_q_index = torch.argmax(future_qvalues)\n",
    "        \n",
    "        target_qvalue = self.get_target_qvalue(reward, future_qvalues[0][future_q_index])\n",
    "        target_qvalues = pred_qvalues\n",
    "        target_qvalues[0][int(pred_q_index)] = target_qvalue\n",
    "\n",
    "        # tstep_history.append((current_state, ep_prev_action, reward, future_state, terminated))\n",
    "        tstep_history.append((\"tstep\"))\n",
    "        return tstep_history, goal_reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "a62e8578-d795-433d-a2d9-1c07a2d2a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, batch_size, n_eps, n_tsteps):\n",
    "        self.n_eps = n_eps\n",
    "        self.n_tsteps = n_tsteps\n",
    "        self.batch_size = batch_size\n",
    "        self.eps_size = 0\n",
    "        self.replay_buffer = [[[] for t in range(self.n_tsteps)] for eps in range(self.n_eps)]\n",
    "\n",
    "    def add(self, episode, t, tstep_history):\n",
    "        self.replay_buffer[episode][t].append(tstep_history)\n",
    "\n",
    "    def can_sample(self): \n",
    "        return self.eps_size >= self.batch_size\n",
    "\n",
    "    def least_filled_t(self, ids):\n",
    "        least_filled = np.inf\n",
    "        count = 0\n",
    "        for id in ids:\n",
    "            for tstep_history in x[id]:\n",
    "                if tstep_history != []:\n",
    "                   count += 1\n",
    "            if least_filled > count:\n",
    "                least_filled = count\n",
    "            count = 0\n",
    "        return least_filled\n",
    "            \n",
    "    def sample_batch(self): \n",
    "        assert self.can_sample()       \n",
    "        if self.eps_size == self.batch_size:\n",
    "            return self.replay_buffer[:self.batch_size][:self.least_filled_t(list(range(self.eps_size)))]   \n",
    "        ids = np.random.choice(self.eps_size, size=self.batch_size, replace=False)\n",
    "        return [self.replay_buffer[id][:self.least_filled_t(ids)] for id in ids] # returns random episode ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "id": "1189623f-b7d9-473d-8cc1-32300fa5bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner():\n",
    "    def __init__(self, grid, batch_size, n_eps, n_tsteps):\n",
    "        self.grid = grid\n",
    "        self.n_tsteps = n_tsteps\n",
    "        self.n_eps = n_eps\n",
    "        self.batch_size = batch_size if batch_size < self.n_eps else self.n_eps\n",
    "        self.n_batches = 1\n",
    "        self.replay_buffer = ReplayBuffer(self.batch_size, self.n_eps, self.n_tsteps)   \n",
    "        self.update_freq = 2\n",
    "        self.target_model = AgentModel(self.grid.agent_in_shape, self.grid.agent.embed_dim, len(Actions))\n",
    "        self.update_agent_model() # initialize agent and target model to be the same\n",
    "        self.goal_reached_info = []\n",
    "        self.alpha = 0.01\n",
    "        self.loss = nn.HuberLoss()\n",
    "        self.optimizer = torch.optim.Adam([{'params': self.target_model.parameters(), 'lr': self.alpha}])\n",
    "\n",
    "    def episode(self, num_episode):\n",
    "        tstep_history = []\n",
    "        goal_reached = False\n",
    "        vizualize = True\n",
    "        t = 0\n",
    "        self.grid.reset()\n",
    "        while not goal_reached and t < self.n_tsteps: \n",
    "            tstep_history, goal_reached = self.grid.agent.step()\n",
    "            self.replay_buffer.add(num_episode, t, tstep_history)\n",
    "            t += 1 \n",
    "            if goal_reached: \n",
    "                self.goal_reached_info.append(f\"Goal reached at episode: {num_episode}, timestep: {t}\")\n",
    "                print(\"Goal Reached!\")\n",
    "            if vizualize:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Epsiode: {num_episode} Timestep: {t}\\n{self.grid.vizualize_grid()}\")\n",
    "        print(\"Episode Finished\")\n",
    "\n",
    "    def batch_generator(self):\n",
    "        batches = []\n",
    "        for i in range(self.n_batches):\n",
    "            batches.append(self.replay_buffer.sample_batch())  \n",
    "        return iter(batches)\n",
    "\n",
    "    def update_agent_model(self):\n",
    "       target_weights = self.target_model.state_dict()\n",
    "       self.grid.agent.local_model.load_state_dict(target_weights)\n",
    "        \n",
    "    def run_episodes(self): \n",
    "        for episode in range(self.n_eps):\n",
    "            self.episode(episode)  \n",
    "            self.replay_buffer.eps_size += 1\n",
    "            if episode % self.update_freq == 0:\n",
    "                self.update_agent_model()\n",
    "            if self.replay_buffer.can_sample():\n",
    "                batch_generator = self.batch_generator()\n",
    "                self.train(batch_generator)  \n",
    "        \n",
    "    def train(self, batch_generator):\n",
    "        target_hidden_states = torch.zeros((self.grid.agent.embed_dim,)) \n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        for inputs, targets in batch_generator:  \n",
    "            self.optimizer.zero_grad()  \n",
    "            outputs, _ = self.target_model(inputs, target_hidden_states)  \n",
    "            loss = self.loss(outputs, targets) \n",
    "            loss.backward() \n",
    "            self.optimizer.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "id": "95ae5e19-c471-4d5f-8f05-f68ae0ba50ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GridEnv(rows=6, cols=4, n_agents=1)\n",
    "qlearner = QLearner(grid=g, batch_size=2, n_eps=10, n_tsteps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "id": "1a6755a1-f08a-4821-8d77-31354042f426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsiode: 1 Timestep: 10\n",
      ". . . .\n",
      ". A . .\n",
      ". G . B\n",
      ". . . .\n",
      ". . . .\n",
      ". . . .\n",
      "Episode Finished\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[780], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gen \u001b[38;5;241m=\u001b[39m \u001b[43mqlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_episodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[778], line 53\u001b[0m, in \u001b[0;36mQLearner.run_episodes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39mcan_sample():\n\u001b[1;32m     52\u001b[0m     batch_generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_generator()\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_generator\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[778], line 60\u001b[0m, in \u001b[0;36mQLearner.train\u001b[0;34m(self, batch_generator)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m batch_generator:  \n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \n\u001b[0;32m---> 60\u001b[0m     outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_hidden_states\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     61\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(outputs, targets) \n\u001b[1;32m     62\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward() \n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[649], line 14\u001b[0m, in \u001b[0;36mAgentModel.forward\u001b[0;34m(self, inputs, hidden_in)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, hidden_in): \n\u001b[0;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m) \n\u001b[1;32m     15\u001b[0m     h_in \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(hidden_in, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim))\n\u001b[1;32m     16\u001b[0m     hidden_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(x, h_in)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "gen = qlearner.run_episodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37178d8-3949-41f9-8e75-bff498d15139",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
